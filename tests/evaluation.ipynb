{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embedding Model Evaluation for Activity Similarity Search\n",
    "\n",
    "This notebook evaluates the performance of an embedding model for semantic similarity search of activity descriptions. The evaluation uses a Qdrant vector database and measures how well the model can match paraphrased activities to their original versions.\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **MRR (Mean Reciprocal Rank)**: Average of reciprocal ranks of first correct matches\n",
    "- **Hits@K**: Proportion of queries where the correct answer is among the first K results\n",
    "\n",
    "## Configuration\n",
    "The notebook supports various noise levels to simulate text spelling errors..\n"
   ],
   "id": "10ae9b41bbd73ff1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Vector Database Initialization\n",
    "\n",
    "This section sets up the components required for vector search:\n",
    "\n",
    "- **Embedding Model**: Converts text into numerical vectors. The embedding model (EM) can be loaded using Hugging Faceâ€™s Sentence Transformers library, allowing you to easily switch between different pre-trained models for experimentation and testing.\n",
    "- **Qdrant Client**: Establishes a connection to the Qdrant vector database, which efficiently stores and retrieves vector embeddings.\n",
    "- **Vector Store**: Provides an interface for similarity searches, wrapping the Qdrant client to handle vector insertion, querying, and database management.\n",
    "- **Noise Configuration**: You can adjust the `noise_error_rate` variable to simulate noise in the embeddings."
   ],
   "id": "3bbca7af471d91fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "import json\n",
    "\n",
    "noise_error_rate = 0.0\n",
    "\n",
    "# Embedding model for semantic similarity search\n",
    "embedding_model_used = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Create collection name based on model name\n",
    "collection_name = embedding_model_used.replace(\"/\", \"-\")\n",
    "\n",
    "# Embedding Model Setup\n",
    "model_kwargs = {\"trust_remote_code\": True}\n",
    "encode_kwargs = {\n",
    "    \"normalize_embeddings\": False,\n",
    "}\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_used,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "# Qdrant Vector Database Setup\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Determine vector dimensions\n",
    "sample_embedding = embedding_model.embed_query(\"Test\")\n",
    "vector_size = len(sample_embedding)\n",
    "\n",
    "print(f\"Vector Dimensions: {vector_size}\")\n",
    "print(f\"Model Used: {embedding_model_used}\")\n",
    "print(f\"Collection Name: {collection_name}\")\n",
    "print(f\"Noise Level: {noise_error_rate}\")\n"
   ],
   "id": "c13cd86b7c2500ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Similarity Search Evaluation\n",
    "\n",
    "This section performs the main evaluation:\n",
    "\n",
    "1. **Data Loading**: Loads ground truth data with original activities and their paraphrases\n",
    "2. **Similarity Search**: Performs similarity search for each paraphrase\n",
    "3. **Ranking Assessment**: Determines the position of the correct answer in search results\n",
    "4. **Metrics Calculation**: Calculates MRR (Mean Reciprocal Rank) and Hits@K values\n",
    "\n",
    "### Evaluation Logic:\n",
    "- Each paraphrase is used as a query\n",
    "- The vector DB returns the top-10 most similar activities\n",
    "- We measure at which position (rank) the original activity is found\n",
    "- A lower rank (position 1) is better than a high rank (position 10)\n"
   ],
   "id": "2a20065c21148cd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(f\"../utils/activities_with_synonyms_merged_noise_{noise_error_rate}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    merged_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(merged_data)} activity groups\")\n",
    "\n",
    "total_queries = 0\n",
    "reciprocal_ranks = []\n",
    "hits_at_k = {1: 0, 2: 0, 3: 0, 5: 0, 10: 0}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nStarting evaluation for model '{embedding_model_used}'...\")\n",
    "\n",
    "for entry in tqdm(merged_data, desc=\"Evaluating activity synonyms\"):\n",
    "    original = entry[\"original_activity\"]\n",
    "    paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "    for synonym in paraphrases:\n",
    "        search_results = vector_store.similarity_search_with_score(synonym, k=10)\n",
    "        matched = False\n",
    "        rank = None\n",
    "\n",
    "        # Search through results for the original activity\n",
    "        for i, (doc, distance) in enumerate(search_results):\n",
    "            if doc.page_content == original:\n",
    "                rank = i + 1\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        total_queries += 1\n",
    "\n",
    "        # Calculate reciprocal rank\n",
    "        reciprocal_ranks.append(1 / rank if matched else 0)\n",
    "\n",
    "        # Update Hits@K counters\n",
    "        for k in hits_at_k:\n",
    "            if matched and rank <= k:\n",
    "                hits_at_k[k] += 1\n",
    "\n",
    "        top_match_text = search_results[0][0].page_content\n",
    "        similarity = 1 - search_results[0][1]\n",
    "\n",
    "        results.append({\n",
    "            \"Original Activity\": original,\n",
    "            \"Paraphrased Activity\": synonym,\n",
    "            \"Found Activity\": top_match_text,\n",
    "            \"Similarity\": round(similarity, 4),\n",
    "            \"Exact_Match\": \"true\" if top_match_text == original else \"false\"\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "csv_filename = f\"evaluation_results/noise_{noise_error_rate}/{collection_name}.csv\"\n",
    "print(f\"\\nSaving detailed results to: {csv_filename}\")\n",
    "\n",
    "# Save all detailed results to CSV file\n",
    "os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "        \"Paraphrased Activity\", \"Found Activity\", \"Original Activity\", \"Similarity\", \"Exact_Match\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "mrr = sum(reciprocal_ranks) / total_queries\n",
    "\n",
    "print(f\"\\nEvaluation Results for '{embedding_model_used}':\")\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"MRR (Mean Reciprocal Rank): {mrr:.4f}\")\n",
    "print(\"\\nHits@K Metrics:\")\n",
    "for k in sorted(hits_at_k):\n",
    "    accuracy = hits_at_k[k] / total_queries\n",
    "    print(f\"   Hits@{k}: {hits_at_k[k]}/{total_queries} = {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDetailed results saved: {csv_filename}\")\n"
   ],
   "id": "c8984a24fd4e9bf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Export Summary Statistics\n",
    "\n",
    "This final section exports a compact summary of all important metrics to a CSV file. This enables easy comparison between different models and configurations.\n",
    "\n",
    "### Update Logic:\n",
    "- If results for the current model already exist, they are updated\n",
    "- New models are added to the existing summary\n",
    "- The CSV file serves as a central overview of all evaluations\n",
    "\n",
    "### Exported Metrics:\n",
    "- Model name and vector dimensions\n",
    "- Total number of tested paraphrases\n",
    "- MRR and all Hits@K values\n"
   ],
   "id": "90a2c3a145de7bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "summary_csv_path = f\"evaluation_results_noise_{noise_error_rate}.csv\"\n",
    "print(f\"Summary file: {summary_csv_path}\")\n",
    "\n",
    "# Create dictionary with all important metrics for current model\n",
    "summary_row = {\n",
    "    \"Model Name\": embedding_model_used,\n",
    "    \"Dimension\": vector_size,\n",
    "    \"Total Synonyms\": total_queries,\n",
    "    \"MRR\": round(mrr, 4),\n",
    "    \"Hits@1\": round(hits_at_k[1] / total_queries, 4),\n",
    "    \"Hits@2\": round(hits_at_k[2] / total_queries, 4),\n",
    "    \"Hits@3\": round(hits_at_k[3] / total_queries, 4),\n",
    "    \"Hits@5\": round(hits_at_k[5] / total_queries, 4),\n",
    "    \"Hits@10\": round(hits_at_k[10] / total_queries, 4),\n",
    "}\n",
    "\n",
    "summary_fieldnames = list(summary_row.keys())\n",
    "summary_data = []\n",
    "if os.path.exists(summary_csv_path):\n",
    "    print(\"Loading existing summary...\")\n",
    "    with open(summary_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        summary_data = list(reader)\n",
    "    print(f\"Found {len(summary_data)} existing entries\")\n",
    "else:\n",
    "    print(\"Creating new summary file...\")\n",
    "\n",
    "# Update or add model entry\n",
    "updated = False\n",
    "for i, row in enumerate(summary_data):\n",
    "    # Search for existing entry for current model\n",
    "    if row[\"Model Name\"] == embedding_model_used:\n",
    "        print(f\"Updating existing entry for '{embedding_model_used}'\")\n",
    "        summary_data[i] = summary_row  # Replace existing entry\n",
    "        updated = True\n",
    "        break\n",
    "\n",
    "# If no existing entry found, add new one\n",
    "if not updated:\n",
    "    print(f\"Adding new entry for '{embedding_model_used}'\")\n",
    "    summary_data.append(summary_row)\n",
    "\n",
    "# Save updated summary\n",
    "print(f\"Saving updated summary...\")\n",
    "with open(summary_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_data)\n",
    "\n",
    "print(f\"Summary successfully updated: {summary_csv_path}\")\n",
    "print(f\"Total models in summary: {len(summary_data)}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"Evaluation for '{embedding_model_used}' completed!\")\n",
    "print(f\"Best metric (Hits@1): {hits_at_k[1] / total_queries:.2%}\")\n",
    "print(f\"MRR Score: {mrr:.4f}\")"
   ],
   "id": "19d0ca94c8c3d7b3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
